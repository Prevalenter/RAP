from utils import utils
import TD3
import torch
import gym
import argparse
import random
import os
import sys
sys.path.append('..')

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)




self.time_rl = 0
            parser = argparse.ArgumentParser()
            parser.add_argument("--policy", default="TD3")  # Policy name (TD3, DDPG or OurDDPG)
            parser.add_argument("--env", default="Gear1-v0")  # Gear1 Robot_axis6
            parser.add_argument("--seed", default=0, type=int)  # Sets Gym, PyTorch and Numpy seeds
            parser.add_argument("--start_timesteps", default=10e3, type=int)  # Time steps initial random policy is used
            parser.add_argument("--eval_freq", default=1e4, type=int)  # How often (time steps) we evaluate
            parser.add_argument("--max_timesteps", default=5e5, type=int)  # Max time steps to run environment
            parser.add_argument("--expl_noise", default=0.05)  # Std of Gaussian exploration noise
            parser.add_argument("--batch_size", default=256, type=int)  # Batch size for both actor and critic
            parser.add_argument("--discount", default=0.95)  # Discount factor
            parser.add_argument("--tau", default=0.005)  # Target network update rate
            parser.add_argument("--policy_noise", default=0.1)  # 0.2 Noise added to target policy during critic update
            parser.add_argument("--noise_clip", default=0.5)  # Range to clip target policy noise
            parser.add_argument("--policy_freq", default=2, type=int)  # Frequency of delayed policy updates
            parser.add_argument("--save_model", default=True,
                                action="store_true")  # Save model and optimizer parameters
            parser.add_argument("--load_model",
                                default="")  # Model load file name, "" doesn't load, "default" uses file_name
            self.args = parser.parse_args()

            self.file_name = f"{self.args.policy}_{self.args.env}_{self.args.seed}"

            if not os.path.exists("./results"):
                os.makedirs("./results")

            if self.args.save_model and not os.path.exists("./models"):
                os.makedirs("./models")

            self.env = gym.make(self.args.env, renders=True)

            self.env.seed(self.args.seed)
            torch.manual_seed(self.args.seed)
            np.random.seed(self.args.seed)

            torch.cuda.manual_seed(self.args.seed)
            torch.cuda.manual_seed_all(self.args.seed)
            random.seed(self.args.seed)

            self.env.action_space.seed(self.args.seed)
            self.env.observation_space.seed(self.args.seed)

            self.state_dim = self.env.observation_space.shape[0]
            self.action_dim = self.env.action_space.shape[0]
            self.max_action = float(self.env.action_space.high[0])
            kwargs = {
                "state_dim": self.state_dim,
                "action_dim": self.action_dim,
                "max_action": self.max_action,
                "discount": self.args.discount,
                "tau": self.args.tau,
            }

            if self.args.policy == "TD3":
                # Target policy smoothing is scaled wrt the action scale
                kwargs["policy_noise"] = self.args.policy_noise * self.max_action
                kwargs["noise_clip"] = self.args.noise_clip * self.max_action
                kwargs["policy_freq"] = self.args.policy_freq
                kwargs["seed"] = self.args.seed
                self.policy = TD3.TD3(**kwargs)

            self.replay_buffer = utils.ReplayBuffer(self.state_dim, self.action_dim)

            self.evaluations = []

            self.reward_list = []

            self.state, self.done = self.env.reset(target_p=self.x_r[:2]), False
            self.episode_reward = 0
            self.episode_timesteps = 0
            self.episode_num = 0




                    if self.prepared:

            self.episode_timesteps += 1

            # Select action randomly or according to policy
            if self.time_rl < self.args.start_timesteps:

                action = self.env.action_space.sample()
                # if t%300==1: print('action', action)
            else:
                self.policy.actor.eval()
                action = (
                        self.policy.select_action(np.array(self.state))
                        + np.random.normal(0, self.max_action * self.args.expl_noise, size=self.action_dim)
                ).clip(-self.max_action, self.max_action)
                self.policy.actor.train()

            next_state, reward, self.done, _ = self.env.step(action)
            done_bool = float(self.done) if self.episode_timesteps < self.env._max_episode_steps else 0

            # Store data in replay buffer
            self.replay_buffer.add(self.state, action, next_state, reward, done_bool)

            self.state = next_state
            self.episode_reward += reward

            if self.done:
                self.reward_list.append(reward)
                np.save(f"./results/reward_list_{self.file_name}", self.reward_list)

            # Train agent after collecting sufficient data
            if self.time_rl >= self.args.start_timesteps:
                self.policy.train(self.replay_buffer, self.args.batch_size)

            if self.done:
                # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True
                print(
                    f"Total T: {self.time_rl + 1} Episode Num: {self.episode_num + 1} Episode T: {self.episode_timesteps} Reward: {self.episode_reward:.3f}, timesteps: {self.env.timesteps}")
                # Reset environment

                self.reset()
                self.state, self.done = self.env.reset(target_p=self.x_r[:2]), False
                self.episode_reward = 0
                self.episode_timesteps = 0
                self.episode_num += 1

                if self.episode_num > 2500:
                    self.timer_stage12.stop()

            # Evaluate episode
            if (self.time_rl + 1) % self.args.eval_freq == 0:
                self.evaluations.append(self.eval_policy(self.policy, self.args.env, self.args.seed, t))
                np.save(f"./results/{self.file_name}", self.evaluations)
                if self.args.save_model: self.policy.save(f"./models/{self.file_name}")

            self.time_rl += 1
            pass



                def eval_policy(policy, env_name, seed, timesteps, eval_episodes=5):
        eval_env = gym.make(env_name)
        eval_env.seed(seed + 100)
        eval_env.timesteps = timesteps

        policy.actor.eval()

        avg_reward = 0.
        for _ in range(eval_episodes):
            state, done = eval_env.reset(), False
            while not done:
                action = policy.select_action(np.array(state))
                state, reward, done, _ = eval_env.step(action)
                avg_reward += reward

        policy.actor.train()

        avg_reward /= eval_episodes

        print("---------------------------------------")
        print(f"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}")
        print("---------------------------------------")
        return avg_reward




# if self.env.prepared:
        #     parser = argparse.ArgumentParser()
        #     parser.add_argument("--policy", default="TD3")  # Policy name (TD3, DDPG or OurDDPG)
        #     parser.add_argument("--env", default="Gear1-v0")  # Gear1 Robot_axis6
        #     parser.add_argument("--seed", default=0, type=int)  # Sets Gym, PyTorch and Numpy seeds
        #     parser.add_argument("--start_timesteps", default=10e3, type=int)  # Time steps initial random policy is used
        #     parser.add_argument("--eval_freq", default=1e4, type=int)  # How often (time steps) we evaluate
        #     parser.add_argument("--max_timesteps", default=5e5, type=int)  # Max time steps to run environment
        #     parser.add_argument("--expl_noise", default=0.05)  # Std of Gaussian exploration noise
        #     parser.add_argument("--batch_size", default=256, type=int)  # Batch size for both actor and critic
        #     parser.add_argument("--discount", default=0.95)  # Discount factor
        #     parser.add_argument("--tau", default=0.005)  # Target network update rate
        #     parser.add_argument("--policy_noise", default=0.1)  # 0.2 Noise added to target policy during critic update
        #     parser.add_argument("--noise_clip", default=0.5)  # Range to clip target policy noise
        #     parser.add_argument("--policy_freq", default=2, type=int)  # Frequency of delayed policy updates
        #     parser.add_argument("--save_model", default=True, action="store_true")  # Save model and optimizer parameters
        #     parser.add_argument("--load_model", default="")  # Model load file name, "" doesn't load, "default" uses file_name
        #     args = parser.parse_args()
        #
        #     file_name = f"{args.policy}_{args.env}_{args.seed}"
        #     print("---------------------------------------")
        #     print(f"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}")
        #     print("---------------------------------------")
        #
        #     if not os.path.exists("./results"):
        #         os.makedirs("./results")
        #
        #     if args.save_model and not os.path.exists("./models"):
        #         os.makedirs("./models")
        #
        #     self.env.seed(args.seed)
        #     torch.manual_seed(args.seed)
        #     np.random.seed(args.seed)
        #
        #     torch.cuda.manual_seed(args.seed)
        #     torch.cuda.manual_seed_all(args.seed)
        #     random.seed(args.seed)
        #
        #     state_dim = self.env.observation_space.shape[0]
        #     action_dim = self.env.action_space.shape[0]
        #     max_action = float(self.env.action_space.high[0])
        #
        #     kwargs = {
        #         "state_dim": state_dim,
        #         "action_dim": action_dim,
        #         "max_action": max_action,
        #         "discount": args.discount,
        #         "tau": args.tau,
        #     }
        #
        #     # Initialize policy
        #     if args.policy == "TD3":
        #         # Target policy smoothing is scaled wrt the action scale
        #         kwargs["policy_noise"] = args.policy_noise * max_action
        #         kwargs["noise_clip"] = args.noise_clip * max_action
        #         kwargs["policy_freq"] = args.policy_freq
        #         kwargs["seed"] = args.seed
        #         policy = TD3.TD3(**kwargs)
        #
        #     if args.load_model != "":
        #         policy_file = file_name if args.load_model == "default" else args.load_model
        #         policy.load(f"./models/{policy_file}")
        #
        #     replay_buffer = utils.ReplayBuffer(state_dim, action_dim)
        #
        #     state, done = self.env.x_r[:2] , False
        #     episode_reward = 0
        #     episode_timesteps = 0
        #     episode_num = 0
        #     evaluations = []
        #     reward_list = []
        #
        #     for t in range(int(args.max_timesteps)):
        #
        #         episode_timesteps += 1
        #
        #         # Select action randomly or according to policy
        #         if t < args.start_timesteps:
        #
        #             action = self.env.action_space.sample()
        #             # if t%300==1: print('action', action)
        #         else:
        #             policy.actor.eval()
        #             action = (
        #                     policy.select_action(np.array(state))
        #                     + np.random.normal(0, max_action * args.expl_noise, size=action_dim)
        #                     ).clip(-max_action, max_action)
        #             policy.actor.train()
        #
        #         print('action', action)
        #
        #         # Perform action
        #         print('state', state)
                # next_state, reward, done, _ = self.env.step(action)
                # done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0
                #
                # # Store data in replay buffer
                # replay_buffer.add(state, action, next_state, reward, done_bool)
                #
                # state = next_state
                # episode_reward += reward
                #
                # if done:
                #     reward_list.append(reward)
                #     np.save(f"./results/reward_list_{file_name}", reward_list)
                #
                # # Train agent after collecting sufficient data
                # if t >= args.start_timesteps:
                #     policy.train(replay_buffer, args.batch_size)
                #
                # if done:
                #     # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True
                #     print(f"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}, timesteps: {env.timesteps}")
                #     # Reset environment
                #     state, done = self.env.reset(), False
                #     episode_reward = 0
                #     episode_timesteps = 0
                #     episode_num += 1
                #
                #     if episode_num > 2500:
                #         break
                #
                # # Evaluate episode
                # if (t + 1) % args.eval_freq == 0:
                #     evaluations.append(replay_buffer.eval_policy(policy, args.env, args.seed, t))
                #     np.save(f"./results/{file_name}", evaluations)
                #     if args.save_model: policy.save(f"./models/{file_name}")